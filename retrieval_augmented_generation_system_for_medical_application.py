# -*- coding: utf-8 -*-
"""Retrieval-Augmented Generation System for Medical Application

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hF79RvwDjt7yCOaK_tKa15SJ6bd4huo0
"""

!pip install datasets transformers

from tqdm.notebook import tqdm
import numpy as np
import pandas as pd

#Dataset

!pip install langchain

with open("train.txt", "r") as f:
    data = f.read()

data[:100]

from langchain.docstore.document import Document as LangchainDocument

raw_database = LangchainDocument(page_content=data)

MARKDOWN_SEPARATORS = [
    "\n#{1,6} ",
    "'''\n",
    "\n\\*\\*\\**\n",
    "\n---+\n",
    "\n___+\n",
    "\n\n",
    "\n",
    " ",
    "",
]

from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    separators=MARKDOWN_SEPARATORS,
    chunk_size=1000,
    chunk_overlap=100,
    )

processed_data = splitter.split_documents([raw_database])

processed_data[0]

!pip install langchain_community
!pip install sentence_transformers

"""**Tokenizing/Vectorizing the dataset**"""

from langchain_community.embeddings import HuggingFaceEmbeddings
model_name = "thenlper/gte-large"

embedding_model = HuggingFaceEmbeddings(
    model_name=model_name,
    multi_process=True,
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}, #Set 'True' for cosine singularity
)

sample_vector = embedding_model.embed_query("Hello World")
print(len(sample_vector))

!pip install pinecone-client

from pinecone import Pinecone

pc = Pinecone(api_key="3e61f584-0a90-44cc-bdac-11f8feffd8fb")
index = pc.Index("lab-rag-index")

index_description = index.describe_index_stats()
index_dimension = 1024

data_to_add = []

for i, entry in tqdm(enumerate(processed_data[:5])):
    text = entry.page_content
    vector = embedding_model.embed_query(text)


    data_to_add.append({
        "id": "vec_{}".format(i),
        "values": vector,
        "metadata": {"text": text}
    })

index.upsert(data_to_add, namespace="ns1")

"""**Loading a LLM**"""

from transformers import pipeline
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_name = "HuggingFaceH4/zephyr-7b-beta"
#Zephyr is a series of language models that are trained to act as helpful assistants.

!pip uninstall bitsandbytes -y
!pip uninstall accelerate -y
!pip install bitsandbytes
!pip install accelerate

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(model_name)

llm_model = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    do_sample=True,
    temperature=0.2,
    max_new_tokens=500
)

llm_model("Hey what's up?")

"""**Prompting the model**"""

prompt = """
<|system|>
You are a helpful assistant that answers on medical questions based on the real information provided from different sources and in the context.
Give the rational and well written response. If you don't have proper info in the context, answer "I don't know"
Respond only to the question asked.

<|user|>
Context:
{}
---
Here is the question you need to answer.

Question: {}
<|assistant|>
"""

user_input = input("User: ")

vectorized_input = embedding_model.embed_query(user_input)

context = index.query(
    namespace="ns1",
    vector=vectorized_input,
    top_k=1,
    include_metadata=True
)

answer = llm_model(prompt.format(context['matches'][0]['metadata']['text'], user_input))

print("AI response: ", answer[0]['generated_text'])

